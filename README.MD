# NLP Learning Cycle ğŸš€  
A Practical Project to Understand RNN, LSTM, and Transformer Limitations

## ğŸ“Œ Project Overview
This project is a **learning-oriented NLP project** designed to demonstrate **why different sequence models exist** and **when each one fails or succeeds**.

Instead of directly jumping to advanced models (like Transformers), we follow a **learning cycle**:
- Start with simple models
- Observe their limitations
- Introduce improved architectures step by step

The goal is **understanding through practice**, not just achieving high accuracy.

---

## ğŸ§  Learning Cycle Concept

Small Dataset + Short Sequences

â†“

RNN works (appears good)

â†“
Longer Sequences / Larger Data

â†“
RNN fails

â†“

LSTM improves

â†“

Very Large Data / Long Dependencies

â†“

LSTM struggles

â†“

Transformer succeeds


Each experiment in this repository represents **one step in this cycle**.

---

## ğŸ“ Project Structure
NLP/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ smalldataset/
â”‚       â””â”€â”€ load_imdb.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ rnn.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ exp1_rnn_small.py
â”‚
â”œâ”€â”€ main.py
â””â”€â”€ README.md
## ğŸ“Š Dataset

**IMDb Movie Reviews Dataset**  
- Binary sentiment classification (positive / negative)
- Real-world text data
- Variable-length reviews

Why IMDb?
- Standard NLP dataset
- Suitable for sequence modeling
- Allows controlled experiments with short and long sequences

The dataset is loaded using `torchtext` to keep preprocessing simple and reproducible.

---

## ğŸ§ª Experiments Description

### Experiment 1 â€“ RNN (Small Dataset)
- IMDb reviews
- Truncated sequences
- Shows that vanilla RNN can appear effective

âš ï¸ This setup is intentionally simplified and not realistic.

---

### ğŸ”¹ Experiment 2: RNN with Long Sequences
**File:** `exp2_rnn_long.py`

- Same dataset
- Longer sequences
- No architectural changes

Expected outcome:
- Training instability
- Poor performance

This experiment demonstrates the **vanishing gradient problem** in RNNs.

---

### ğŸ”¹ Experiment 3: LSTM on the Same Data
**File:** `exp3_lstm.py`

- Same data as Experiment 2
- Replace RNN with LSTM

Expected outcome:
- Improved performance
- More stable training

This shows how **gated mechanisms** help model long-term dependencies.

---

### ğŸ”¹ Experiment 4: Transformer Model
**File:** `exp4_transformer.py`

- Large dataset
- Long sequences
- Self-attention instead of recurrence

Expected outcome:
- Better scalability
- Faster training
- Strong performance

This explains why Transformers dominate modern NLP.

---

## ğŸ›  Technologies Used
- Python
- PyTorch
- torchtext
- Git & GitHub

---

## ğŸ¯ Key Takeaways
- Model choice depends on data characteristics
- RNNs are limited by sequential processing
- LSTMs mitigate but do not eliminate these issues
- Transformers solve long-range dependency problems efficiently

---

## ğŸ“Œ Final Note
This repository is designed as a **learning journey**, not a benchmark competition.
Understanding *why models fail* is more important than achieving perfect accuracy.